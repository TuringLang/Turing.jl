using StatsFuns
using DistributionsAD
using Bijectors
using Bijectors: TransformedDistribution
using Random: AbstractRNG, GLOBAL_RNG

update(d::TuringDiagMvNormal, Î¼, Ïƒ) = TuringDiagMvNormal(Î¼, Ïƒ)
update(td::TransformedDistribution, Î¸...) = transformed(update(td.dist, Î¸...), td.transform)

# TODO: add these to DistributionsAD.jl and remove from here
Distributions.params(d::TuringDiagMvNormal) = (d.m, d.Ïƒ)

import StatsBase: entropy
function entropy(d::TuringDiagMvNormal)
    T = eltype(d.Ïƒ)
    return (DistributionsAD.length(d) * (T(log2Ï€) + one(T)) / 2 + sum(log.(d.Ïƒ)))
end

import Bijectors: bijector
function bijector(model::Model; sym_to_ranges::Val{sym2ranges} = Val(false)) where {sym2ranges}
    varinfo = Turing.VarInfo(model)
    num_params = sum([size(varinfo.metadata[sym].vals, 1)
                      for sym âˆˆ keys(varinfo.metadata)])

    dists = vcat([varinfo.metadata[sym].dists for sym âˆˆ keys(varinfo.metadata)]...)

    num_ranges = sum([length(varinfo.metadata[sym].ranges)
                      for sym âˆˆ keys(varinfo.metadata)])
    ranges = Vector{UnitRange{Int}}(undef, num_ranges)
    idx = 0
    range_idx = 1

    # ranges might be discontinuous => values are vectors of ranges rather than just ranges
    sym_lookup = Dict{Symbol, Vector{UnitRange{Int}}}()
    for sym âˆˆ keys(varinfo.metadata)
        sym_lookup[sym] = Vector{UnitRange{Int}}()
        for r âˆˆ varinfo.metadata[sym].ranges
            ranges[range_idx] = idx .+ r
            push!(sym_lookup[sym], ranges[range_idx])
            range_idx += 1
        end

        idx += varinfo.metadata[sym].ranges[end][end]
    end

    bs = inv.(bijector.(tuple(dists...)))

    if sym2ranges
        return Stacked(bs, ranges), (; collect(zip(keys(sym_lookup), values(sym_lookup)))...)
    else
        return Stacked(bs, ranges)
    end
end

"""
    meanfield(model::Model)

Creates a mean-field approximation with multivariate normal as underlying distribution.
"""
function meanfield(model::Model)
    # setup
    varinfo = Turing.VarInfo(model)
    num_params = sum([size(varinfo.metadata[sym].vals, 1)
                      for sym âˆˆ keys(varinfo.metadata)])

    dists = vcat([varinfo.metadata[sym].dists for sym âˆˆ keys(varinfo.metadata)]...)

    num_ranges = sum([length(varinfo.metadata[sym].ranges)
                      for sym âˆˆ keys(varinfo.metadata)])
    ranges = Vector{UnitRange{Int}}(undef, num_ranges)
    idx = 0
    range_idx = 1
    for sym âˆˆ keys(varinfo.metadata)
        for r âˆˆ varinfo.metadata[sym].ranges
            ranges[range_idx] = idx .+ r
            range_idx += 1
        end
        
        # append!(ranges, [idx .+ r for r âˆˆ varinfo.metadata[sym].ranges])
        idx += varinfo.metadata[sym].ranges[end][end]
    end

    # initial params
    Î¼ = randn(num_params)
    Ïƒ = softplus.(randn(num_params))

    # construct variational posterior
    d = TuringDiagMvNormal(Î¼, Ïƒ)
    bs = inv.(bijector.(tuple(dists...)))
    b = Stacked(bs, ranges)

    return transformed(d, b)
end

"""
    ADVI(samples_per_step = 1, max_iters = 1000)

Automatic Differentiation Variational Inference (ADVI) for a given model.
"""
struct ADVI{AD} <: VariationalInference{AD}
    samples_per_step # number of samples used to estimate the ELBO in each optimization step
    max_iters        # maximum number of gradient steps used in optimization
end

ADVI(args...) = ADVI{ADBackend()}(args...)
ADVI() = ADVI(1, 1000)

alg_str(::ADVI) = "ADVI"


function vi(model::Model, alg::ADVI; optimizer = TruncatedADAGrad())
    q = meanfield(model)
    return vi(model, alg, q; optimizer = optimizer)
end

# TODO: make more flexible, allowing other types of `q`
function vi(model, alg::ADVI, q::TransformedDistribution{<:TuringDiagMvNormal}; optimizer = TruncatedADAGrad())
    Turing.DEBUG && @debug "Optimizing ADVI..."
    # Initial parameters for mean-field approx
    Î¼, Ïƒs = params(q)
    Î¸ = vcat(Î¼, invsoftplus.(Ïƒs))

    # Optimize
    optimize!(elbo, alg, q, model, Î¸; optimizer = optimizer)

    # Return updated `Distribution`
    Î¼, Ï‰ = Î¸[1:length(q)], Î¸[length(q) + 1:end]
    return update(q, Î¼, softplus.(Ï‰))
end

function vi(model, alg::ADVI, q, Î¸_init; optimizer = TruncatedADAGrad())
    Turing.DEBUG && @debug "Optimizing ADVI..."
    Î¸ = copy(Î¸_init)
    optimize!(elbo, alg, q, model, Î¸; optimizer = optimizer)

    # If `q` is a mean-field approx we use the specialized `update` function
    if q isa TransformedDistribution{<:TuringDiagMvNormal}
        Î¼, Ï‰ = Î¸[1:length(q)], Î¸[length(q) + 1:end]
        return update(q, Î¼, softplus.(Ï‰))
    else
        # Otherwise we assume it's a mapping Î¸ â†’ q
        return q(Î¸)
    end
end


function optimize(elbo::ELBO, alg::ADVI, q, model, Î¸_init; optimizer = TruncatedADAGrad())
    Î¸ = copy(Î¸_init)
    
    if model isa Model
        optimize!(elbo, alg, q, make_logjoint(model), Î¸; optimizer = optimizer)
    else
        # `model` assumed to be callable z â†¦ p(x, z)
        optimize!(elbo, alg, q, model, Î¸; optimizer = optimizer)
    end

    return Î¸
end

"""
    make_logjoint(model; weight = 1.0)

Constructs the logjoint as a function of latent variables, i.e. the map z â†’ p(x âˆ£ z) p(z).

The weight used to scale the likelihood, e.g. when doing stochastic gradient descent one needs to
use `DynamicPPL.MiniBatch` context to run the `Model` with a weight `num_total_obs / batch_size`.
"""
function make_logjoint(model; weight = 1.0)
    # setup
    ctx = DynamicPPL.MiniBatchContext(
        DynamicPPL.DefaultContext(),
        weight
    )
    varinfo = Turing.VarInfo(model, ctx)

    function logÏ€(z)
        varinfo = VarInfo(varinfo, SampleFromUniform(), z)
        model(varinfo)
        
        return varinfo.logp
    end

    return logÏ€
end

function logjoint(model, varinfo, z)
    varinfo = VarInfo(varinfo, SampleFromUniform(), z)
    model(varinfo)

    return varinfo.logp
end

function (elbo::ELBO)(alg::ADVI, q, logÏ€, Î¸, num_samples; kwargs...)
    return elbo(GLOBAL_RNG, alg, q, logÏ€, Î¸, num_samples; kwargs...)
end


function (elbo::ELBO)(
    rng::AbstractRNG,
    alg::ADVI,
    q,
    model::Model,
    Î¸::AbstractVector{<:Real},
    num_samples;
    weight = 1.0,
    kwargs...
)   
    return elbo(rng, alg, q, make_logjoint(model; weight = weight), Î¸, num_samples; kwargs...)
end

function (elbo::ELBO)(
    alg::ADVI,
    q::TransformedDistribution{<:TuringDiagMvNormal},
    model::Model,
    num_samples;
    kwargs...
)
    # extract the mean-field Gaussian params
    Î¼, Ïƒs = params(q)
    Î¸ = vcat(Î¼, invsoftplus.(Ïƒs))

    return elbo(alg, q, model, Î¸, num_samples; kwargs...)
end


function (elbo::ELBO)(
    rng::AbstractRNG,
    alg::ADVI,
    q::TransformedDistribution{<:TuringDiagMvNormal},
    logÏ€::Function,
    Î¸::AbstractVector{<:Real},
    num_samples
)
    num_params = length(q)
    Î¼ = Î¸[1:num_params]
    Ï‰ = Î¸[num_params + 1: end]

    # update the variational posterior
    q = update(q, Î¼, softplus.(Ï‰))

    #   ð”¼_q(z)[log p(xáµ¢, z)]
    # = âˆ« log p(xáµ¢, z) q(z) dz
    # = âˆ« log p(xáµ¢, f(Ï•)) q(f(Ï•)) |det J_f(Ï•)| dÏ•   (since change of variables)
    # = âˆ« log p(xáµ¢, f(Ï•)) qÌƒ(Ï•) dÏ•                   (since q(f(Ï•)) |det J_f(Ï•)| = qÌƒ(Ï•))
    # = ð”¼_qÌƒ(Ï•)[log p(xáµ¢, z)]

    #   ð”¼_q(z)[log q(z)]
    # = âˆ« q(f(Ï•)) log (q(f(Ï•))) |det J_f(Ï•)| dÏ•     (since q(f(Ï•)) |det J_f(Ï•)| = qÌƒ(Ï•))
    # = ð”¼_qÌƒ(Ï•) [log q(f(Ï•))]
    # = ð”¼_qÌƒ(Ï•) [log qÌƒ(Ï•) - log |det J_f(Ï•)|]
    # = ð”¼_qÌƒ(Ï•) [log qÌƒ(Ï•)] - ð”¼_qÌƒ(Ï•) [log |det J_f(Ï•)|]
    # = - â„(qÌƒ(Ï•)) - ð”¼_qÌƒ(Ï•) [log |det J_f(Ï•)|]

    # Finally, the ELBO is given by
    # ELBO = ð”¼_q(z)[log p(xáµ¢, z)] - ð”¼_q(z)[log q(z)]
    #      = ð”¼_qÌƒ(Ï•)[log p(xáµ¢, z)] + ð”¼_qÌƒ(Ï•) [log |det J_f(Ï•)|] + â„(qÌƒ(Ï•))

    # If f: supp(p(z | x)) â†’ â„ then
    # ELBO = ð”¼[log p(x, z) - log q(z)]
    #      = ð”¼[log p(x, fâ»Â¹(zÌƒ)) + logabsdet(J(fâ»Â¹(zÌƒ)))] + â„(qÌƒ(zÌƒ))
    #      = ð”¼[log p(x, z) - logabsdetjac(J(f(z)))] + â„(qÌƒ(zÌƒ))

    # But our `forward(q)` is using fâ»Â¹: â„ â†’ supp(p(z | x)) going forward â†’ `+ logjac`
    _, z, logjac, _ = forward(rng, q)
    res = (logÏ€(z) + logjac) / num_samples

    res += entropy(q.dist)
    
    for i = 2:num_samples
        _, z, logjac, _ = forward(rng, q)
        res += (logÏ€(z) + logjac) / num_samples
    end

    return res
end

function (elbo::ELBO)(
    rng::AbstractRNG,
    alg::ADVI,
    getq::Function,
    logÏ€::Function,
    Î¸::AbstractVector{<:Real},
    num_samples
)
    # Update the variational posterior
    q = getq(Î¸)

    # ELBO computation
    _, z, logjac, _ = forward(rng, q)
    res = (logÏ€(z) + logjac) / num_samples

    res += entropy(q.dist)
    
    for i = 2:num_samples
        _, z, logjac, _ = forward(rng, q)
        res += (logÏ€(z) + logjac) / num_samples
    end

    return res
end

# function (elbo::ELBO)(
#     rng::AbstractRNG,
#     alg::ADVI,
#     getq::Function,
#     logÏ€::Function,
#     Î¸::AbstractVector{<:Real},
#     estimator::AbstractEstimator;
#     weight = 1.0
# )
