using StatsFuns
using Turing.Core: update
using Bijectors

"""
    ADVI(samples_per_step = 1, max_iters = 1000)

Automatic Differentiation Variational Inference (ADVI) for a given model.
"""
struct ADVI{AD} <: VariationalInference{AD}
    samples_per_step # number of samples used to estimate the ELBO in each optimization step
    max_iters        # maximum number of gradient steps used in optimization
end

ADVI(args...) = ADVI{ADBackend()}(args...)
ADVI() = ADVI(1, 1000)

alg_str(::ADVI) = "ADVI"

function logdensity(model, varinfo, z)
    varinfo = VarInfo(varinfo, SampleFromUniform(), z)
    model(varinfo)

    return varinfo.logp
end

function (elbo::ELBO)(
    alg::ADVI,
    q::TransformedDistribution{<: TuringDiagNormal},
    model::Model,
    Î¸::AbstractVector{T},
    num_samples,
    weight = 1.0
) where T <: Real
    # setup
    varinfo = Turing.VarInfo(model)

    # extract params
    num_params = length(q)
    Î¼ = Î¸[1:num_params]
    Ï‰ = Î¸[num_params + 1: end]

    # update the variational posterior
    q = update(q, Î¼, softplus.(Ï‰))

    # rescaling due to loglikelihood weight and samples used
    c = weight / num_samples

    # If f: supp(p(z | x)) â†’ â„ then
    # ELBO = ð”¼[log p(x, z) - log q(z)]
    #      = ð”¼[log p(x, fâ»Â¹(zÌƒ)) + logabsdet(J(fâ»Â¹(zÌƒ)))] + â„(qÌƒ(zÌƒ))
    #      = ð”¼[og p(x, z) - logabsdetjac(J(f(z)))] + â„(qÌƒ(zÌƒ))
    _, z, logjac, _ = forward(q)
    res = (logdensity(model, varinfo, z) - logjac) * c

    res += entropy(q)
    
    for i = 2:num_samples
        _, z, logjac, _ = forward(q)
        res += (logdensity(model, varinfo, z) - logjac) * c
    end

    return res
end

function (elbo::ELBO)(
    alg::VariationalInference,
    q::TransformedDistribution{<: TuringDiagNormal},
    model::Model,
    num_samples
)
    # extract the mean-field Gaussian params
    Î¼, Ïƒs = params(q)
    Î¸ = vcat(Î¼, invsoftplus.(Ïƒs))

    return elbo(alg, q, model, Î¸, num_samples)
end

